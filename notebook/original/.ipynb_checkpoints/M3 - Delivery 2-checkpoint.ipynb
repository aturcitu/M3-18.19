{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# M3 - Delivery 2\n",
    "\n",
    "## Created By: Laura M., Andreu J.R., Aitor S."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main objective in this second delivery of the M3-Machine Learning for Computer Vision Module is to solve an image classification problem by using the Bag of words classification, Cross-validation and Spatial Pyramids (BoW framework).\n",
    "Also, Histogram Intersection Kernel Support Vector Mchines (SVM) have been used for the image classification problem. Each image was split into different blocks, which were represented by the Scale Invariant Feature Transform (SIFT) descriptors. After this, a k-means cluster method has been applied to cluster the SIFT descriptors into groups, each of them represented by a visual keyword (label). The following method consists in counting the number of descriptors in each image and to construct a histogram for, later, create a Histogram Intersection Kernel. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hereunder, the different Python files have been enclosed. So, this notebook is a clustering of all the different code created for the second delivery of the M3 module. \n",
    "\n",
    "It is important to remark that the working directory -path- in which the MIT_split dataset images are stored -as train and test-, should be used in order to execute the following code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Config.py\n",
    "\n",
    "In this file, the variables used can be modified in order to execute the code with the desired values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default values from week 1\n",
    "def variables():\n",
    "    \n",
    "    # Determines total number of kps in an given image (set composed of 256x256px img)\n",
    "    sift_step_size = 20\n",
    "\n",
    "    # List providing scale values to compute at each kp\n",
    "    sift_scale = [20] \n",
    "\n",
    "    # Dense/Normal Sift \n",
    "    dense = True\n",
    "    \n",
    "    # Number of clusters in KMeans, size of codebook (words)\n",
    "    k_codebook = 128\n",
    "    \n",
    "    type_classifier = \"KNN\"\n",
    "\n",
    "    knn_dict =\t{\n",
    "      \"k_classifier\": 5,\n",
    "      \"distance_method\": \"euclidean\",\n",
    "    }\n",
    "    \n",
    "    svm_dict ={\n",
    "        \"kernel\": [\"linear\", \"rbf\", \"poly\", \"precomputed\"],\n",
    "        \"C_linear\": 0.1,\n",
    "        \"C_linear2\": 0.1,\n",
    "        \"C_rbf\": 1,\n",
    "        \"C_poly\": 0.1,\n",
    "        \"C_inter\": 1,\n",
    "        \"gamma\": 0.001,\n",
    "        \"degree\": 1,\n",
    "    }\n",
    "    \n",
    "    # Number of pyramid levels used\n",
    "    pyramid_level = 0\n",
    "    # CrosValidation division kfold\n",
    "    number_splits = 3\n",
    "    # Intersection Kernel for SVN \n",
    "    intersection = False\n",
    "    # Distance method used in order to normalize bincounts for the BoW\n",
    "    norm_method = \"L2\"\n",
    "\n",
    "    return (sift_step_size, sift_scale, dense, k_codebook, type_classifier, \n",
    "            svm_dict, knn_dict, pyramid_level, number_splits, intersection, norm_method)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pyramid_words.py\n",
    "\n",
    "The file pyramid_words.py consists in the creation of a method that splits the image in several divisions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "\n",
    "def pyramid_visual_word(pyramid_descriptors, codebook, k_codebook, test_descriptors):\n",
    "    visual_words_test = []\n",
    "    \n",
    "    for pyramid_level in pyramid_descriptors:\n",
    "        \n",
    "        for im_pyramid, j in zip(pyramid_level, np.arange(len(pyramid_level))):\n",
    "            words_hist = np.array([])\n",
    "            \n",
    "            for sub_im in im_pyramid:\n",
    "\n",
    "                sub_words = codebook.predict(sub_im)\n",
    "                sub_words_hist = np.bincount(sub_words, minlength=k_codebook)\n",
    "                sub_words_hist = normalize(sub_words_hist.reshape(-1,1), norm= 'l2', axis=0).reshape(1,-1)\n",
    "                words_hist = np.append(words_hist, sub_words_hist) \n",
    "                \n",
    "            if(len(visual_words_test)<len(test_descriptors)):\n",
    "               visual_words_test.append(words_hist)\n",
    "               \n",
    "            else:\n",
    "               visual_words_test[j] = np.append(visual_words_test[j], words_hist)\n",
    "    \n",
    "    return np.array(visual_words_test, dtype='f')             \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifier.py\n",
    "\n",
    "The classifier.py file contains different methods used to compute a histogram intersection kernel with using a SVM classifier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "def init_classifier_knn(knn_param):\n",
    "\n",
    "    return KNeighborsClassifier(\n",
    "            n_neighbors=knn_param[\"k_classifier\"], \n",
    "            n_jobs=-1, \n",
    "            metric=knn_param[\"distance_method\"])\n",
    "\n",
    "def init_classifier_svm(svm_param):\n",
    "    \n",
    "    models = (\n",
    "            (svm.SVC(kernel=svm_param[\"kernel\"][0], C=svm_param[\"C_linear\"]), \"linear1\"),\n",
    "            (svm.LinearSVC(C=svm_param[\"C_linear2\"]), \"linear2\"),\n",
    "            (svm.SVC(kernel=svm_param[\"kernel\"][1], gamma=svm_param[\"gamma\"], C=svm_param[\"C_rbf\"]), \"rbf\"),\n",
    "            (svm.SVC(kernel=svm_param[\"kernel\"][2], degree=svm_param[\"degree\"], C=svm_param[\"C_poly\"]), \"poly\"),\n",
    "            (svm.SVC(kernel=svm_param[\"kernel\"][3], C=svm_param[\"C_inter\"]), \"inter\")\n",
    "            )\n",
    "        \n",
    "    return models\n",
    "\n",
    "\n",
    "def histogram_intersection(set1, set2):\n",
    "\n",
    "    inter = np.zeros( (len(set1),len(set2)) )\n",
    "    \n",
    "    for x, hist1 in enumerate(set1):\n",
    "        for y,hist2 in enumerate(set2):\n",
    "            minima = np.minimum(hist1, hist2)\n",
    "            inter[x][y] = sum(minima)     \n",
    "            \n",
    "    return inter\n",
    "    \n",
    "    \n",
    "def compute_intersection_kernel(data_test, data_train):\n",
    "\n",
    "    scld = StandardScaler().fit(data_train)\n",
    "    scaled_train = scld.transform(data_train)\n",
    "    scaled_test = scld.transform(data_test)\n",
    "\n",
    "    return histogram_intersection(scaled_train, scaled_test)\n",
    "\n",
    "\n",
    "def compute_regular_kernel(data_test, data_train):\n",
    "    return data_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization.py\n",
    "\n",
    "This file is used to visualize the results by plotting them. Two functions are included: the first one creates an accuracy vs. time plot while the second one creates the plot of the confusion matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import itertools\n",
    "import numpy as np\n",
    "\n",
    "def plot_accuracy_vs_time(x, y1, y2, feature_name, title):\n",
    "    \"\"\"\n",
    "    This function plots a doble axis figure.\n",
    "    Feature name and title can be modified to be plot\n",
    "    \"\"\"  \n",
    "    fig, ax1 =plt.subplots()\n",
    "    color = 'tab:red'\n",
    "    ax1.set_xlabel(feature_name)\n",
    "    ax1.set_ylabel('accuracy', color=color)\n",
    "    ax1.plot(x, y1, color=color)\n",
    "    ax1.tick_params(axis='y', labelcolor=color)\n",
    "    \n",
    "    ax2 = ax1.twinx()  # instantiate a second axes that shares the same x-axis\n",
    "    color = 'tab:blue'\n",
    "    ax2.set_ylabel('time (s)', color=color)  \n",
    "    ax2.plot(x,y2, color=color)\n",
    "    ax2.tick_params(axis='y', labelcolor=color)\n",
    "    \n",
    "    fig.tight_layout()  # otherwise the right y-label is slightly clipped\n",
    "    plt.title(title)\n",
    "    plt.show()     \n",
    "\n",
    "def plot_confusion_matrix(cm, classes, normalize=False, \n",
    "                          title='Confusion matrix', cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    plt.figure()\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BagofVisualWords_python3x.py\n",
    "\n",
    "This is the main file of this delivery. It contains several methods such as the detectors, the descriptors pyramid, the BOW or the cross validation computation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import sys\n",
    "\n",
    "from pyramid_words import pyramid_visual_word\n",
    "from classifier import init_classifier_svm, init_classifier_knn\n",
    "from classifier import compute_intersection_kernel, compute_regular_kernel\n",
    "from visualization import plot_accuracy_vs_time, plot_confusion_matrix\n",
    "from config import variables\n",
    "\n",
    "def open_pkl(pkl_file):\n",
    "    \"\"\"\n",
    "    This function opens pkl files providing file name on WD.\n",
    "    \"\"\"\n",
    "    with open(pkl_file, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    return data\n",
    "\n",
    "\n",
    "def compute_detector(sift_step_size, sift_scale, n_features=300):\n",
    "    \"\"\"\n",
    "    Computes Sift detector object.\n",
    "    Computes mesh of KPs using a custom step size and scale value(s).\n",
    "    Points are shifted by sift_step_size/2 in order to avoid points on \n",
    "    image borders\n",
    "    \"\"\"\n",
    "    SIFTdetector = cv2.xfeatures2d.SIFT_create(nfeatures=n_features)\n",
    "\n",
    "    if not isinstance(sift_scale, list):\n",
    "        sift_scale = [sift_scale]\n",
    "\n",
    "    kpt = [cv2.KeyPoint(x, y, scale) for y in\n",
    "           range(int(sift_step_size / 2) - 1, 256 - int(sift_step_size / 2), sift_step_size) for x in\n",
    "           range(int(sift_step_size / 2) - 1, 256 - int(sift_step_size / 2), sift_step_size) for scale in sift_scale]\n",
    "\n",
    "    return (SIFTdetector, kpt)\n",
    "\n",
    "\n",
    "def compute_des_pyramid(dataset_desc, pyramid_level, kpt, img_px=256):\n",
    "    \"\"\"\n",
    "    Computes Pyramid divison of the kp descriptors dataset\n",
    "    It uses KPs values to descriminate to which level each descriptor belongs\n",
    "    \"\"\"\n",
    "    div_level = int(2 ** (pyramid_level))\n",
    "    pyramid_res = img_px / div_level\n",
    "    pyramid_desc = []\n",
    "\n",
    "    for image_desc in dataset_desc:\n",
    "        im_pyramid_desc = []\n",
    "        # axis 0 divisions\n",
    "        for n in range(1, div_level + 1):\n",
    "            # axis 1 divisions\n",
    "            for m in range(1, div_level + 1):\n",
    "                sub_desc = []\n",
    "                for kp_desc, kp in zip(image_desc, kpt):\n",
    "                    x, y = kp.pt\n",
    "                    # sub resolution area\n",
    "                    if (((n - 1) * pyramid_res <= x < n * pyramid_res) and\n",
    "                            ((m - 1) * pyramid_res <= y < m * pyramid_res)):\n",
    "                        sub_desc.append(kp_desc)\n",
    "\n",
    "                im_pyramid_desc.append(np.array(sub_desc, dtype='f'))\n",
    "\n",
    "        pyramid_desc.append(im_pyramid_desc)\n",
    "\n",
    "    return pyramid_desc\n",
    "\n",
    "\n",
    "def compute_BOW(train_images_filenames, dense, SIFTdetector, kpt,\n",
    "                k_codebook, pyramid_level, norm_method):\n",
    "    train_descriptors = []\n",
    "    # Compute SIFT descriptors for whole DS \n",
    "    for filename in train_images_filenames:\n",
    "        ima = cv2.imread(filename)\n",
    "        gray = cv2.cvtColor(ima, cv2.COLOR_BGR2GRAY)\n",
    "        if dense:\n",
    "            (_, des) = SIFTdetector.compute(gray, kpt)\n",
    "        else:\n",
    "            (_, des) = SIFTdetector.detectAndCompute(gray, None)\n",
    "            # Creates a list with all the descriptors\n",
    "        train_descriptors.append(des)\n",
    "\n",
    "    # Descriptors are clustered with KMeans (whole image, e.g pyramid_level = 0)\n",
    "    descriptors = np.vstack(train_descriptors)\n",
    "\n",
    "    codebook = MiniBatchKMeans(n_clusters=k_codebook, batch_size=k_codebook * 20,\n",
    "                               compute_labels=False, reassignment_ratio=10 ** -4,\n",
    "                               random_state=42)\n",
    "    codebook.fit(descriptors)\n",
    "\n",
    "    # Pyramid Representation of n Levels\n",
    "    pyramid_descriptors = []\n",
    "\n",
    "    while pyramid_level >= 0:\n",
    "        pyramid_descriptors.append(compute_des_pyramid(train_descriptors, pyramid_level, kpt))\n",
    "        pyramid_level -= 1\n",
    "\n",
    "    # Create visual words with normalized bins for each image and subimage\n",
    "    # After individually normalized, bins are concatenated for each image\n",
    "\n",
    "    visual_words = pyramid_visual_word(pyramid_descriptors, codebook, k_codebook, train_descriptors)\n",
    "\n",
    "    return codebook, visual_words\n",
    "\n",
    "\n",
    "def test_BOW(test_images_filenames, dense, SIFTdetector, kpt, k_codebook, pyramid_level, codebook):\n",
    "    test_descriptors = []\n",
    "\n",
    "    for filename in test_images_filenames:\n",
    "        ima = cv2.imread(filename)\n",
    "        gray = cv2.cvtColor(ima, cv2.COLOR_BGR2GRAY)\n",
    "        if dense:\n",
    "            (_, des) = SIFTdetector.compute(gray, kpt)\n",
    "        else:\n",
    "            (_, des) = SIFTdetector.detectAndCompute(gray, None)\n",
    "\n",
    "        test_descriptors.append(des)\n",
    "\n",
    "    # Pyramid Representation of n Levels            \n",
    "    pyramid_descriptors = []\n",
    "\n",
    "    while pyramid_level >= 0:\n",
    "        pyramid_descriptors.append(compute_des_pyramid(test_descriptors, pyramid_level, kpt))\n",
    "        pyramid_level -= 1\n",
    "\n",
    "    # Create visual words with normalized bins for each image and subimage\n",
    "    # After individually normalized, bins are concatenated for each image\n",
    "    visual_words_test = pyramid_visual_word(pyramid_descriptors, codebook, k_codebook, test_descriptors)\n",
    "\n",
    "    return visual_words_test\n",
    "\n",
    "\n",
    "def compute_accuracy_labels(test_labels, train_labels, test_data, clf):\n",
    "\n",
    "    accuracy = 100 * clf.score(test_data, test_labels)\n",
    "    predicted_labels = clf.predict(test_data)\n",
    "    unique_labels = list(set(train_labels))\n",
    "\n",
    "    # Compute confusion matrix\n",
    "    cnf_matrix = confusion_matrix(test_labels, predicted_labels, labels=unique_labels)\n",
    "\n",
    "    return accuracy, cnf_matrix, unique_labels\n",
    "\n",
    "\n",
    "def cross_validation(skf, X, y, sift_scale, sift_step_size, k_codebook, dense, pyramid_level, norm_method, compute_kernel):\n",
    "    splits_accuracy = []\n",
    "    splits_time = []\n",
    "\n",
    "    for number, (train_index, test_index) in enumerate(skf.split(X, y)):\n",
    "        x_train, x_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "        start = time.time()\n",
    "\n",
    "        (SIFTdetector, kpt) = compute_detector(sift_step_size, sift_scale)\n",
    "\n",
    "        (codebook, visual_words) = compute_BOW(x_train, dense, SIFTdetector, kpt,\n",
    "                                               k_codebook, pyramid_level, norm_method)\n",
    "        bow_time = time.time()\n",
    "\n",
    "        # Compute kernel for classifier\n",
    "        # If not intersection, kernelMatrix = visual_words\n",
    "        kernel_matrix = compute_kernel(visual_words, visual_words)\n",
    "\n",
    "        classifier.fit(kernel_matrix, y_train)\n",
    "\n",
    "        visual_words_test = test_BOW(x_test, dense, SIFTdetector, kpt, k_codebook,\n",
    "                                     pyramid_level, codebook)\n",
    "\n",
    "        # Compute kernel for classifier\n",
    "        # If not intersection, kernelMatrix = visual_words\n",
    "        kernel_matrix_test = compute_kernel(visual_words_test, visual_words)\n",
    "\n",
    "        accuracy, cnf_matrix, unique_labels = compute_accuracy_labels(y_test, y_train, \n",
    "                                                                      kernel_matrix_test, \n",
    "                                                                      classifier)\n",
    "\n",
    "        class_time = time.time()\n",
    "        ttime = class_time - start\n",
    "\n",
    "        # Add accuracy for each validation step\n",
    "        splits_accuracy.append(accuracy)\n",
    "        splits_time.append(ttime)\n",
    "\n",
    "        print(\"\\nAccuracy for split\", number, \":\", accuracy, \"\\nTotal Time: \", class_time - start,\n",
    "              \"\\nBOW Time: \", bow_time - start, \"\\nClassification Time: \", class_time - bow_time)\n",
    "\n",
    "        # Plot normalized confusion matrix\n",
    "        np.set_printoptions(precision=2)\n",
    "        plot_confusion_matrix(cnf_matrix, classes=unique_labels,\n",
    "                              normalize=True,\n",
    "                              title='Normalized confusion matrix')\n",
    "\n",
    "    return splits_accuracy, splits_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Playing starts here\n",
    "Witih all the functions neede defined, it is time to start finding our best values for our values and the differences between differents methods in order to classify these images. First of all, based on the feedback received from last weed, SIFT parameters are going to be revesited, now with a proper cross-validation scheme.\n",
    "\n",
    "Step size is iterated over using power distribution (2, 4, 16 ...). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loads Default Variables\n",
    "(sift_step_size, sift_scale, dense, k_codebook, type_classifier, \n",
    " svm_dict, knn_dict, pyramid_level, number_splits, intersection, norm_method) = variables()\n",
    "\n",
    "# Prepare files from DS for training and creates Folds\n",
    "train_images = open_pkl('train_images_filenames.dat')\n",
    "train_labels = open_pkl('train_labels.dat')\n",
    "\n",
    "X = np.array(train_images)\n",
    "y = np.array(train_labels)\n",
    "\n",
    "skf = StratifiedKFold(n_splits=number_splits, random_state=42, shuffle=True)\n",
    "\n",
    "# Loads KNN classifier\n",
    "classifier = init_classifier_knn(knn_dict)\n",
    "\n",
    "accuracy_list = []\n",
    "time_list = []\n",
    "\n",
    "swiping_variable = [2**n for n in range(8)]\n",
    "\n",
    "for sift_step_size in swiping_variable:\n",
    "    accuracy_validation, time_validation = cross_validation(skf, X, y, sift_scale, sift_step_size, k_codebook,\n",
    "                                                            dense, pyramid_level, norm_method, compute_regular_kernel)\n",
    "    # Append for the different testing values\n",
    "    time_list.append(np.average(accuracy_validation))\n",
    "    accuracy_list.append(np.average(time_validation))\n",
    "    \n",
    "plot_accuracy_vs_time(swiping_variable, accuracy_list, time_list,\n",
    "                      feature_name='Number of SIFT scales', title=\"DSIFT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
